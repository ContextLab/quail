{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pyrororobocop</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from analysis import recall_matrix\n",
    "\n",
    "class Pyro(object):\n",
    "    \"\"\"\n",
    "    Data object for the pyrec package\n",
    "\n",
    "    This class contains free recall data and metadata that will be used by pyrec.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    pres : pd.DataFrame\n",
    "        Dataframe containing the presented words.  Each row represents the presented words for a given list and each column\n",
    "        represents a list. The cells should be lowercase words. The index will be a multi-index, where the first level reprensents the subject number\n",
    "        and the second level represents the list number\n",
    "    \n",
    "    features : pd.DataFrame\n",
    "        Dataframe containing the features for presented words.  Each row represents the presented words for a given list and each column\n",
    "        represents a list. The cells should be a dictionary of features, where the keys are the name of the features, and the values are the feature values.\n",
    "        The index will be a multi-index, where the first level reprensents the subject number and the second level represents the list number\n",
    "        \n",
    "    rec : pd.DataFrame\n",
    "        Dataframe containing the words recalled.  Each row represents the recalled words for a given list and each column\n",
    "        represents a list.  Each row represents the recalled words for a given list and each column\n",
    "        represents a list. The cells should be lowercase words. The index will be a multi-index, where the first level reprensents the subject number\n",
    "        and the second level represents the list number\n",
    "    \n",
    "    meta : dict (optional)\n",
    "        Meta data about the study (i.e. version, description, date, etc.) can be saved here\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, pres=pd.DataFrame(), features=pd.DataFrame(), rec=pd.DataFrame(), meta={}):\n",
    "\n",
    "        self.pres=pres\n",
    "        self.features=features\n",
    "        self.rec=rec\n",
    "        self.meta=meta\n",
    "        self._recall_mtx = recall_matrix(self.pres, self.rec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Pyro at 0x1086095d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pyro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kirstenziman/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from __future__ import division\n",
    "import re\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.ma as ma\n",
    "from itertools import izip_longest\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load data in dataframe</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db_url = \"sqlite:///participants.db\"\n",
    "table_name = 'turkdemo'\n",
    "data_column_name = 'datastring'\n",
    "\n",
    "# boilerplace sqlalchemy setup\n",
    "engine = create_engine(db_url)\n",
    "metadata = MetaData()\n",
    "metadata.bind = engine\n",
    "table = Table(table_name, metadata, autoload=True)\n",
    "\n",
    "# make a query and loop through\n",
    "s = table.select()\n",
    "rows = s.execute()\n",
    "\n",
    "data = []\n",
    "for row in rows:\n",
    "    data.append(row[data_column_name])\n",
    "    \n",
    "# Now we have all participant datastrings in a list.\n",
    "# Let's make it a bit easier to work with:\n",
    "\n",
    "# parse each participant's datastring as json object\n",
    "# and take the 'data' sub-object\n",
    "data = [json.loads(part)['data'] for part in data if part is not None]\n",
    "\n",
    "# insert uniqueid field into trialdata in case it wasn't added\n",
    "# in experiment:\n",
    "for part in data:\n",
    "    for record in part:\n",
    "#         print(record)\n",
    "        if type(record['trialdata']) is list:\n",
    "\n",
    "            record['trialdata'] = {record['trialdata'][0]:record['trialdata'][1]}\n",
    "        record['trialdata']['uniqueid'] = record['uniqueid']\n",
    "        \n",
    "# flatten nested list so we just have a list of the trialdata recorded\n",
    "# each time psiturk.recordTrialData(trialdata) was called.\n",
    "def isNotNumber(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return True\n",
    "\n",
    "data = [record['trialdata'] for part in data for record in part]\n",
    "\n",
    "# filter out fields that we dont want using isNotNumber function\n",
    "filtered_data = [{k:v for (k,v) in part.items() if isNotNumber(k)} for part in data]\n",
    "    \n",
    "# Put all subjects' trial data into a dataframe object from the\n",
    "# 'pandas' python library: one option among many for analysis\n",
    "data_frame = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Track Experiement Number</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_url = \"sqlite:///participants.db\"\n",
    "table_name = 'turkdemo'\n",
    "data_column_name = 'codeversion'\n",
    "\n",
    "# boilerplace sqlalchemy setup\n",
    "engine = create_engine(db_url)\n",
    "metadata = MetaData()\n",
    "metadata.bind = engine\n",
    "table = Table(table_name, metadata, autoload=True)\n",
    "\n",
    "# make a query and loop through\n",
    "s = table.select()\n",
    "rows = s.execute()\n",
    "\n",
    "versions = []\n",
    "for row in rows:\n",
    "    versions.append(row[data_column_name])\n",
    "    \n",
    "version_col = []\n",
    "for idx,sub in enumerate(data_frame['uniqueid'].unique()):\n",
    "    for i in range(sum(data_frame['uniqueid']==sub)):\n",
    "        version_col.append(versions[idx])\n",
    "data_frame['exp_version']=version_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Word Pool</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in stimulus library\n",
    "wordpool = pd.read_csv('cut_wordpool.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Select Experiment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'debugN8TPWO:debugF1XWCH', u'debugZQ55YL:debug5WQHPC', u'debugWF2JFB:debugPNRZFQ', u'debugQRX0V3:debugFIWAG8', u'debugIAU8V9:debugT1DECK', u'debug02E4FI:debugF7UOXH', u'debugS4GATI:debug2LRP6X', u'debugJAPX2W:debugFZOLSG', u'debugA98B98:debug5H8QRL', u'debugKDM8HT:debugH2I05W', u'debugQS9870:debugKM1SRC', u'debugVFPD79:debugIP75FV', u'debugX84L2K:debugCDN40O', u'debugSU1T93:debugKCB9VM', u'debugE1CAO3:debugONZ2R5', u'debug8DEMRS:debugC55CO6', u'debugXUZA8U:debugMR3K3X']\n"
     ]
    }
   ],
   "source": [
    "subids = list(data_frame[data_frame['listNumber']==15]['uniqueid'].unique())\n",
    "\n",
    "d = dict()\n",
    "for sub in subids:\n",
    "    key = data_frame[data_frame['uniqueid']==sub]['exp_version'].values[0]\n",
    "    if key in d:\n",
    "        d[key].append(sub)\n",
    "    else:\n",
    "        d[key]=[sub]\n",
    "\n",
    "#reaplce these values with the experiment number\n",
    "#three values for the case of experiment 1 only\n",
    "#print (d[\"0.0\"], d['1.0'], d['1.1'])\n",
    "\n",
    "exp1=d[\"0.0\"]+d['1.0']+d['1.1']\n",
    "\n",
    "\n",
    "exp1.remove('debugGPNALW:debugXSJ1FD')\n",
    "exp1.remove('debugLXMXTP:debugJAXRZL')\n",
    "exp1.remove('debugHP65NS:debugLWS9KB')\n",
    "exp1.remove('debugKUWU41:debug9FG9EP')\n",
    "\n",
    "\n",
    "# subids.remove('debug4PXFJG:debug3V9BT9')\n",
    "# subids.remove('debugAD2211:debugB3TKJQ')\n",
    "# subids.remove('debug7XDZDR:debugO8OCCV') # all of the audio files are empty ?!\n",
    "# subids.remove('debugTX7U35:debugZFTPLT')\n",
    "##################\n",
    "\n",
    "print(exp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Processing Functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function takes the data frame and returns subject specific data based on the subid variable\n",
    "def filterData(data_frame,subid):\n",
    "    filtered_stim_data = data_frame[data_frame['stimulus'].notnull() & data_frame['listNumber'].notnull()]\n",
    "    filtered_stim_data = filtered_stim_data[filtered_stim_data['trial_type']=='single-stim']\n",
    "    filtered_stim_data =  filtered_stim_data[filtered_stim_data['uniqueid']==subid]\n",
    "    return filtered_stim_data\n",
    "\n",
    "# this function parses the data creating an array of dictionaries, where each dictionary represents a trial (word presented) along with the stimulus attributes\n",
    "def createStimDict(data):\n",
    "    stimDict = []\n",
    "    for index, row in data.iterrows():\n",
    "        stimDict.append({\n",
    "                'text': str(re.findall('>(.+)<',row['stimulus'])[0]),\n",
    "                'color' : { 'r' : int(re.findall('rgb\\((.+)\\)',row['stimulus'])[0].split(',')[0]),\n",
    "                           'g' : int(re.findall('rgb\\((.+)\\)',row['stimulus'])[0].split(',')[1]),\n",
    "                           'b' : int(re.findall('rgb\\((.+)\\)',row['stimulus'])[0].split(',')[2])\n",
    "                           },\n",
    "                'location' : {\n",
    "                    'top': float(re.findall('top:(.+)\\%;', row['stimulus'])[0]),\n",
    "                    'left' : float(re.findall('left:(.+)\\%', row['stimulus'])[0])\n",
    "                    },\n",
    "                'category' : wordpool['CATEGORY'].iloc[list(wordpool['WORD'].values).index(str(re.findall('>(.+)<',row['stimulus'])[0]))],\n",
    "                'size' : wordpool['SIZE'].iloc[list(wordpool['WORD'].values).index(str(re.findall('>(.+)<',row['stimulus'])[0]))],\n",
    "                'wordLength' : len(str(re.findall('>(.+)<',row['stimulus'])[0])),\n",
    "                'firstLetter' : str(re.findall('>(.+)<',row['stimulus'])[0])[0],\n",
    "                'listnum' : row['listNumber']\n",
    "            })\n",
    "    return stimDict\n",
    "\n",
    "# this function loads in the recall data into an array of arrays, where each array represents a list of words\n",
    "def loadRecallData(subid):\n",
    "    recalledWords = []\n",
    "    for i in range(0,16):\n",
    "        try:\n",
    "            f = open('recall_data/' + subid + '-' + str(i) + '.wav.txt', 'rb')\n",
    "            try:\n",
    "                spamreader = csv.reader(f, delimiter=' ', quotechar='|')\n",
    "            except:\n",
    "                f = open('recall_data/' + subid + '/' + subid + '-' + str(i) + '.wav.txt', 'rb')\n",
    "                spamreader = csv.reader(f, delimiter=' ', quotechar='|')\n",
    "        except (IOError, OSError) as e:\n",
    "            print(e)\n",
    "        for row in spamreader:\n",
    "            recalledWords.append(row[0].split(','))\n",
    "    return recalledWords\n",
    "\n",
    "# this function computes accuracy for a series of lists\n",
    "def computeListAcc(stimDict,recalledWords):\n",
    "    accVec = []\n",
    "    for i in range(0,16):\n",
    "        stim = [stim['text'] for stim in stimDict if stim['listnum']==i]\n",
    "        recalled= recalledWords[i]\n",
    "        \n",
    "        acc = 0\n",
    "        tmpstim = stim[:]\n",
    "        for word in recalled:\n",
    "            if word in tmpstim:\n",
    "                tmpstim.remove(word)\n",
    "                acc+=1\n",
    "        accVec.append(acc/len(stim))\n",
    "    return accVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create presentedWords and recalledWords</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[['OAK', 'JUMPER', 'MARIMBA', 'ROBE', 'COLANDER', 'BLENDER', 'COTTONWOOD', 'EVERGREEN', 'HARMONICA', 'BANJO', 'BELT', 'UKULELE', 'SPRUCE', 'CUP', 'SHIRT', 'MIXER'], ['SAW', 'CLARINET', 'PIANO', 'KAZOO', 'FERRET', 'CHISEL', 'SQUIRREL', 'ENGLAND', 'DRUM', 'LEOPARD', 'CAT', 'BOLTS', 'FRANCE', 'JAPAN', 'PROTRACTOR', 'SWITZERLAND'], ['LOG', 'BROCCOLI', 'CARPET', 'PICKLE', 'GRIDDLE', 'DISH', 'AZALEA', 'STRAINER', 'BUTTERCUP', 'SAUCER', 'FOUNDATION', 'FLOOR', 'ONION', 'ELEVATOR', 'CAULIFLOWER', 'DAHLIA'], ['WASHER', 'WISCONSIN', 'MANGO', 'VANCOUVER', 'VERMONT', 'HAWAII', 'TOKYO', 'JERUSALEM', 'KUMQUAT', 'RULER', 'PHILADELPHIA', 'NECTARINE', 'CALIFORNIA', 'GLUE', 'BANANA', 'NUTS'], ['SCREWS', 'HONEYDEW', 'PINE', 'ELM', 'RASPBERRY', 'HAMMER', 'FIG', 'HICKORY', 'SAXOPHONE', 'PLUM', 'WIRE', 'TRIANGLE', 'MAPLE', 'WEDGE', 'VIOLIN', 'CELLO'], ['LEMON', 'IOWA', 'MICHIGAN', 'POMEGRANATE', 'KANSAS', 'RIB', 'MONKEY', 'RACOON', 'CLEMENTINE', 'MOOSE', 'FLORIDA', 'TONGUE', 'TOOTH', 'PEAR', 'ANTELOPE', 'PANCREAS'], ['POLAND', 'YAMS', 'ETHIOPIA', 'ZEBRA', 'TROMBONE', 'BASS', 'ELEPHANT', 'CARROT', 'ITALY', 'CELERY', 'VIOLA', 'MARACAS', 'PIG', 'LETTUCE', 'LION', 'AUSTRALIA'], ['GLOVES', 'MANTIS', 'MILLIPEDE', 'SYCAMORE', 'BRA', 'BERLIN', 'MOSQUITO', 'RENO', 'CEDAR', 'CHICAGO', 'TROUSERS', 'TEAK', 'SWEATER', 'PALM', 'SYDNEY', 'HORNET'], ['WILLOW', 'EUCALYPTUS', 'BONGOS', 'GUITAR', 'HAND', 'WORM', 'LIP', 'LARVA', 'SCORPION', 'ANKLE', 'ASH', 'POPLAR', 'FINGER', 'MONARCH', 'ACCORDION', 'XYLOPHONE'], ['NARCISSUS', 'KNIFE', 'SUNFLOWER', 'DRESS', 'PANTS', 'LILY', 'SKILLET', 'DAISY', 'MAINE', 'GIRDLE', 'ARIZONA', 'MONTANA', 'TOASTER', 'FORK', 'UNDERWEAR', 'UTAH'], ['MUG', 'ARMS', 'HIP', 'BROILER', 'TAMBOURINE', 'PICCOLO', 'THERMOMETER', 'STOMACH', 'TRUMPET', 'TUBA', 'OVEN', 'DALLAS', 'MONTREAL', 'PELVIS', 'ROME', 'PARIS'], ['OKRA', 'PUMPKIN', 'PENNSYLVANIA', 'OHIO', 'RHINOCEROS', 'RUTABAGA', 'CHRYSALIS', 'TERMITE', 'BEETLE', 'TEXAS', 'ALASKA', 'GINGER', 'LLAMA', 'FOX', 'WASP', 'RABBIT'], ['KIDNEY', 'FOOT', 'SQUASH', 'STOCKHOLM', 'BOMBAY', 'ELBOW', 'GOAT', 'SHEEP', 'OLIVE', 'TORONTO', 'SHOULDER', 'MIAMI', 'ZUCCHINI', 'WOLF', 'AUBERGINE', 'PANTHER'], ['SPINACH', 'CAMISOLE', 'IRAN', 'BLOUSE', 'EGYPT', 'CLOSET', 'CHIMNEY', 'POTATO', 'FURNACE', 'LOBBY', 'GERMANY', 'JACKET', 'SUIT', 'CUBA', 'ARTICHOKE', 'GARLIC'], ['GUAVA', 'GIRAFFE', 'HORSE', 'COW', 'SPATULA', 'DOOR', 'POT', 'HALL', 'CORRIDOR', 'WINDOW', 'KIWI', 'GLASS', 'TANGELO', 'REFRIGERATOR', 'STRAWBERRY', 'MOUSE'], ['KNUCKLE', 'KITCHEN', 'HIPPOPOTAMUS', 'HEART', 'DOG', 'BARN', 'DONKEY', 'TIGER', 'EAR', 'TULIP', 'ROSE', 'FACE', 'CARNATION', 'GAZEBO', 'PETUNIA', 'ALCOVE']]]\n"
     ]
    }
   ],
   "source": [
    "experiment=[]\n",
    "all_recalled=[]\n",
    "all_presented=[]\n",
    "for idx,sub in enumerate(exp1[0:1]):\n",
    "\n",
    "    filteredStimData = filterData(data_frame,sub)\n",
    "    stimDict = createStimDict(filteredStimData)\n",
    "    #get and parse subject's data\n",
    "\n",
    "    presentedWords = [[] for i in range(0,16)]  \n",
    "    for data in stimDict:\n",
    "        presentedWords[int(data['listnum'])].append(data['text'])\n",
    "\n",
    "    recalledWords = loadRecallData(sub)\n",
    "    experiment.append(recall_matrix(presentedWords, recalledWords))\n",
    "    \n",
    "    all_recalled.append(recalledWords)\n",
    "    all_presented.append(presentedWords)\n",
    "    \n",
    "print(len(all_presented))\n",
    "print(all_presented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create Presented DataFrame</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9], [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11], [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12], [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13], [14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14], [15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]]\n",
      "\n",
      "[15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]\n",
      "\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "def pres2pd(all_presented):\n",
    "    \n",
    "    subs = len(all_presented)\n",
    "#   lists = len(all_presented[0])\n",
    "#   words_per = len(all_presented[0][0])\n",
    "    #assumes all subs do equal lists and all lists have equal words\n",
    "    \n",
    "    subs_idx = []\n",
    "    lsts_idx = []\n",
    "    for idx,subs in enumerate(all_presented):\n",
    "        \n",
    "        for idx2,sub in enumerate(subs):\n",
    "            sub_idx = []\n",
    "            lst_idx = []\n",
    "            \n",
    "            for idx3,lst in enumerate(sub):\n",
    "                lst_idx.append(idx3)\n",
    "            \n",
    "            sub_idx = [idx2]*len(lst_idx)\n",
    "            \n",
    "            subs_idx.append(sub_idx)\n",
    "            lsts_idx.append(lst_idx)\n",
    "            \n",
    "    print(subs_idx)\n",
    "    print\n",
    "    print(sub_idx)\n",
    "    print\n",
    "    print(lst_idx)\n",
    "    \n",
    "pres2pd(all_presented)\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
